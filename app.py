import spaces
import os
import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread

# Configuration
MODEL_ID = "somosnlp-hackathon-2025/mistral-7B-ec-es-recetas"
MAX_MAX_NEW_TOKENS = 2048
DEFAULT_MAX_NEW_TOKENS = 512
MAX_INPUT_TOKEN_LENGTH = int(os.getenv("MAX_INPUT_TOKEN_LENGTH", "4096"))

# Global variables
model = None
tokenizer = None

def load_model():
    """Load model and tokenizer"""
    global model, tokenizer
    
    if torch.cuda.is_available():
        print(f"Loading model: {MODEL_ID}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_ID, 
                torch_dtype=torch.float16, 
                device_map="auto",
                trust_remote_code=True
            )
            
            # Set pad token if not present
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                
            print("âœ… Model loaded successfully!")
            return True
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            return False
    else:
        print("âŒ CUDA not available")
        return False

# Load model on startup
model_loaded = load_model()

@spaces.GPU
def generate(
    message: str,
    chat_history: list[tuple],
    max_new_tokens: int = DEFAULT_MAX_NEW_TOKENS,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 50,
    repetition_penalty: float = 1.2,
):
    """Generate response with streaming"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        yield "âŒ Error: Modelo no disponible. Por favor, reinicia la aplicaciÃ³n."
        return
    
    # Convert chat_history format from tuples to messages
    conversation = []
    for user_msg, assistant_msg in chat_history:
        conversation.append({"role": "user", "content": user_msg})
        if assistant_msg:
            conversation.append({"role": "assistant", "content": assistant_msg})
    
    # Add current message
    conversation.append({"role": "user", "content": message})
    
    try:
        # Apply chat template
        input_ids = tokenizer.apply_chat_template(
            conversation, 
            return_tensors="pt",
            add_generation_prompt=True
        )
        
        # Check input length
        if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:
            input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]
            gr.Warning(f"ConversaciÃ³n recortada a {MAX_INPUT_TOKEN_LENGTH} tokens.")
        
        input_ids = input_ids.to(model.device)
        
        # Setup streamer
        streamer = TextIteratorStreamer(
            tokenizer, 
            timeout=30.0, 
            skip_prompt=True, 
            skip_special_tokens=True
        )
        
        # Generation parameters
        generate_kwargs = {
            "input_ids": input_ids,
            "streamer": streamer,
            "max_new_tokens": max_new_tokens,
            "do_sample": True,
            "top_p": top_p,
            "top_k": top_k,
            "temperature": temperature,
            "repetition_penalty": repetition_penalty,
            "pad_token_id": tokenizer.eos_token_id,
            "eos_token_id": tokenizer.eos_token_id,
        }
        
        # Start generation in separate thread
        generation_thread = Thread(target=model.generate, kwargs=generate_kwargs)
        generation_thread.start()
        
        # Stream response
        outputs = []
        try:
            for new_text in streamer:
                outputs.append(new_text)
                yield "".join(outputs)
        except Exception as e:
            yield f"âŒ Error durante la generaciÃ³n: {str(e)}"
        finally:
            generation_thread.join(timeout=1)
            
    except Exception as e:
        yield f"âŒ Error: {str(e)}"

# Create ChatInterface directly (no Blocks wrapper)
demo = gr.ChatInterface(
    fn=generate,
    title="ğŸ½ï¸ Chef Virtual: Patrimonio GastronÃ³mico Ecuatoriano-Colombiano",
    description="Â¡Descubre los sabores tradicionales de Ecuador y Colombia! PregÃºntame sobre recetas, ingredientes y tÃ©cnicas culinarias.",
    chatbot=gr.Chatbot(
        height=500,
        placeholder="Â¡Hola! Soy tu chef virtual especializado en gastronomÃ­a ecuatoriana y colombiana. Â¿En quÃ© puedo ayudarte hoy?",
        avatar_images=(None, "ğŸ½ï¸")
    ),
    textbox=gr.Textbox(
        placeholder="Escribe tu pregunta sobre recetas, ingredientes o tÃ©cnicas culinarias...",
        scale=7
    ),
    additional_inputs=[
        gr.Slider(
            label="Longitud mÃ¡xima de respuesta",
            minimum=100,
            maximum=MAX_MAX_NEW_TOKENS,
            step=50,
            value=DEFAULT_MAX_NEW_TOKENS,
            info="Controla quÃ© tan larga puede ser la respuesta"
        ),
        gr.Slider(
            label="Creatividad (Temperature)",
            minimum=0.1,
            maximum=2.0,
            step=0.1,
            value=0.7,
            info="MÃ¡s alto = respuestas mÃ¡s creativas, mÃ¡s bajo = mÃ¡s conservadoras"
        ),
        gr.Slider(
            label="Diversidad (Top-p)",
            minimum=0.1,
            maximum=1.0,
            step=0.05,
            value=0.9,
            info="Controla la diversidad en la selecciÃ³n de palabras"
        ),
        gr.Slider(
            label="Top-k",
            minimum=1,
            maximum=100,
            step=1,
            value=50,
            info="NÃºmero de opciones de palabras a considerar"
        ),
        gr.Slider(
            label="PenalizaciÃ³n por repeticiÃ³n",
            minimum=1.0,
            maximum=2.0,
            step=0.05,
            value=1.2,
            info="Evita que el modelo repita frases"
        ),
    ],
    examples=[
        ["Â¿CuÃ¡les son los ingredientes principales del locro ecuatoriano?"],
        ["Â¿CÃ³mo se prepara la arepa colombiana tradicional?"],
        ["Dame una receta completa de sancocho de gallina criolla"],
        ["Â¿QuÃ© diferencias hay entre el ceviche ecuatoriano y el peruano?"],
        ["Â¿CÃ³mo hacer empanadas de verde ecuatorianas?"],
        ["Receta de bandeja paisa colombiana paso a paso"],
        ["Â¿QuÃ© postres tÃ­picos puedo hacer con panela?"],
        ["Ingredientes y preparaciÃ³n del encebollado ecuatoriano"],
        ["Â¿CÃ³mo se hace el chocolate santafereÃ±o?"],
        ["Receta de humitas ecuatorianas dulces"],
    ],
    cache_examples=False,
    retry_btn="ğŸ”„ Reintentar",
    undo_btn="â†©ï¸ Deshacer",
    clear_btn="ğŸ—‘ï¸ Limpiar conversaciÃ³n",
    submit_btn="ğŸ“¤ Enviar",
    stop_btn="â¹ï¸ Detener",
    theme=gr.themes.Soft(),
    css="""
    .gradio-container {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    footer {
        display: none !important;
    }
    """
)

if __name__ == "__main__":
    if model_loaded:
        print("ğŸš€ Launching Gradio app...")
        demo.launch(
            share=False,
            show_error=True
        )
    else:
        print("âŒ Failed to load model. Cannot start the app.")